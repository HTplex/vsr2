{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498bb7b5-3414-41a0-9b01-06807cc721a5",
   "metadata": {},
   "source": [
    "1. Find a suitable dataset containing articles textual contents and titles.\n",
    "2. Choose a suitable metric for our task.\n",
    "3. Fine-tune a pre-trained model for title generation on Colab, monitoring the chosen metric on the validation set using TensorBoard, and saving the modelâ€™s checkpoints on Google Drive (so that we can resume training in case Colab shuts down the connection).\n",
    "4. Upload the model on Hugging Face Hub for everyone to use.\n",
    "5. Build an interactive demo with Streamlit and deploy it to Hugging Face Spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0b8f1-f0a4-4011-b839-a9d0f6f7efc5",
   "metadata": {},
   "source": [
    "## 1. scraping dataset \n",
    "https://github.com/codelucas/newspaper\n",
    "\n",
    "passed for now, downloading datasets directly from \n",
    "\n",
    "https://www.kaggle.com/datasets/fabiochiusano/medium-articles\n",
    "\n",
    "to /datasets/medium_articles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538ae84-df9c-4f3f-b11e-cb13e2a8130e",
   "metadata": {},
   "source": [
    "## 2. explore datasets, create training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18da9ea-4a3b-4c26-9b53-e893919d3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw set\n",
    "import pandas\n",
    "raw_dataset_path = \"/data/agent_h/datasets/medium_articles.csv\"\n",
    "tmp_df = pandas.read_csv(raw_dataset_path)\n",
    "dicts = tmp_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e62fa5-0682-4f08-8266-6f1f8e00fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192368\n",
      "{'authors': \"['Ryan Fan']\",\n",
      " 'tags': \"['Mental Health', 'Health', 'Psychology', 'Science', 'Neuroscience']\",\n",
      " 'text': 'Photo by Josh Riemer on Unsplash\\n'\n",
      "         '\\n'\n",
      "         'Merry Christmas and Happy Holidays, everyone!\\n'\n",
      "         '\\n'\n",
      "         'We just wanted everyone to know how much we appreciate everyone and '\n",
      "         'how thankful we are for all our readers and writers here. We '\n",
      "         'wouldnâ€™t be anywhere without you, so thank you all for bringing '\n",
      "         'informative, vulnerable, and important pieces that destigmatize '\n",
      "         'mental illness and mental health.\\n'\n",
      "         '\\n'\n",
      "         'Without further ado, here are ten of our top stories from last week, '\n",
      "         'all of which were curated:\\n'\n",
      "         '\\n'\n",
      "         'â€œJust as the capacity to love and inspire is universal so is the '\n",
      "         'capacity to hate and discourage. Irrespective of gender, race, age '\n",
      "         'or religion none of us are exempt from aggressive proclivities. '\n",
      "         'Those who are narcissistically disordered, and accordingly repress '\n",
      "         'deep seated feelings of inferiority with inflated delusions of '\n",
      "         'grandeur and superiority, are more prone to aggression and violence. '\n",
      "         'They infiltrate our interactions in myriad environments from home, '\n",
      "         'work, school and the cyber world. Hence, bullying does not happen in '\n",
      "         'isolation. Although there is a ringleader she looks to her minions '\n",
      "         'to either sanction her cruelty or look the other way.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œEven though the circumstances that brought me here were sad and '\n",
      "         'challenging, Iâ€™m grateful for how this program has changed my life '\n",
      "         'for the better. I canâ€™t help but imagine what life would be like if '\n",
      "         'everyone learned to accept their powerlessness over other people, '\n",
      "         'prioritize their serenity, and take life one step at a time. Weâ€™ll '\n",
      "         'never know, but Iâ€™d bet the world would be much happier.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œThe prospect of spending a horrible Christmas, locked in on a '\n",
      "         'psychiatric unit, was one of the low points of my life. For weeks, '\n",
      "         'the day room was festooned with cheesy decorations and a sorry pink '\n",
      "         'aluminum tree. All of our â€œactivityâ€ therapies revolved around the '\n",
      "         'holidays. We baked and decorated cookies. We fashioned quick-drying '\n",
      "         'clay into ornaments that turned out to be too heavy for the tree. '\n",
      "         'Crappy Christmas carols were background torture. It was hard to get '\n",
      "         'pissed off at the staff because they were making the best with what '\n",
      "         'they had.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œAlthough I hate to admit it, even if my ex had never betrayed me, I '\n",
      "         'still wouldnâ€™t have been happy. I had set him up for an impossible '\n",
      "         'job â€” to define me and make me whole. If I cannot find peace and '\n",
      "         'contentment within myself, how could anyone else do it for me?â€\\n'\n",
      "         '\\n'\n",
      "         'â€œOn a personal note, significant feelings of loss and sadness can '\n",
      "         'still flare up from time to time. Thatâ€™s only natural; itâ€™s no '\n",
      "         'reason for self-critique. No matter how resilient we purport to be, '\n",
      "         'we are all emotionally vulnerable human beings. Besides, we arenâ€™t '\n",
      "         'talking about some conceptual loss that we can just mechanically '\n",
      "         'compartmentalize away â€” we are talking about the loss of our '\n",
      "         'fathers, mothers, sisters and brothers.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œThe next six weeks will be hard as cases continue to explode and '\n",
      "         'government leadership remains nonexistent. I canâ€™t control any of '\n",
      "         'this. The only thing I can do is take deep breaths, remain vigilant '\n",
      "         'when it comes to limiting exposure to the virus, and let lots of '\n",
      "         'stuff go. I may always be a hypochondriac, but now that I recognize '\n",
      "         'the beast, Iâ€™m hopeful Iâ€™ll be able to tame it.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œFrom anecdotal news reports and informal surveys, there is evidence '\n",
      "         'that for some of us, this pandemic-imposed isolation is a boon '\n",
      "         'rather than a trial. One study on mixed emotions showed that those '\n",
      "         'with lower emotional stability (â€œmoodyâ€ personalities) are actually '\n",
      "         'better at responding to uncertainty.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œEvery day I wish in my heart and soul that I didnâ€™t have ME/CFS. '\n",
      "         'Unfortunately, I do. Itâ€™s a result of a virus I had; 10â€“12 percent '\n",
      "         'of people who experience a serious infection go on to develop ME. '\n",
      "         'Iâ€™ve visualized life without CFS for over a year now; I can smell '\n",
      "         'life without it, I can taste it. Itâ€™s in the smell of the lavender '\n",
      "         'fields that I can no longer run through. Itâ€™s in the taste of the '\n",
      "         'meals from my favorite restaurant that I can no longer walk to. Itâ€™s '\n",
      "         'on the tip of my tongue. Itâ€™s in the potentialities; all the things '\n",
      "         'I could be doing, as a twenty-four year-old, that I canâ€™t. I cannot '\n",
      "         'cross the chasm between the potential and the reality. And thatâ€™s '\n",
      "         'nothing to do with manifestation.â€\\n'\n",
      "         '\\n'\n",
      "         'â€œWhether itâ€™s cabin fever, redundancy, loss, or general Covid '\n",
      "         'anxieties, this year has caused us to be exposed to more uncertainty '\n",
      "         'than ever. Uncertainty creates unease and feelings of stress. Some '\n",
      "         'of us may have taken this year as one to motivate â€” plan dream '\n",
      "         'trips, and prepare and be inspired for what the future could bring. '\n",
      "         'For the rest, it has caused us to become irrational, emotional, and '\n",
      "         'reserved.\\n'\n",
      "         '\\n'\n",
      "         'â€œTo be more self-compassionate is a task that can be tricky because '\n",
      "         'we always want to push ourselves and do better. Without realising '\n",
      "         'it, this can lead to us being self-critical which can have damaging '\n",
      "         'consequences.\\n'\n",
      "         '\\n'\n",
      "         'Itâ€™s important to notice these times when we are harsh because we '\n",
      "         'can easily turn it into self-compassion, which is linked to a better '\n",
      "         'quality of life.â€\\n'\n",
      "         '\\n'\n",
      "         'Merry Christmas and Happy Holidays, everyone!\\n'\n",
      "         '\\n'\n",
      "         'â€” Ryan, Juliette, Marie, and Meredith',\n",
      " 'timestamp': '2020-12-26 03:38:10.479000+00:00',\n",
      " 'title': 'Mental Note Vol. 24',\n",
      " 'url': 'https://medium.com/invisible-illness/mental-note-vol-24-969b6a42443f'}\n"
     ]
    }
   ],
   "source": [
    "# explore data format\n",
    "from pprint import pprint\n",
    "print(len(dicts))\n",
    "sample_dict = dicts[0].copy()\n",
    "sample_dict['text'] = sample_dict['text'][:200]\n",
    "pprint(dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576f678d-549b-445c-b63f-8c2e3161cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agent_h/miniconda3/envs/vsr3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'str'>, <class 'float'>}\n",
      "192361\n"
     ]
    }
   ],
   "source": [
    "# use dataset lib, https://huggingface.co/docs/datasets/en/loading\n",
    "# best way would be raw_data -> process into train -> save as csv or json chunks -> load as dataset\n",
    "import transformers\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "# medium_datasets = load_dataset(\"csv\",\n",
    "#                                data_files=raw_dataset_path)\n",
    "\n",
    "# before using from_list,\n",
    "# need to make sure each key in the list has the same type of value\n",
    "timestamp_types = set([type(x['timestamp']) for x in dicts])\n",
    "print(timestamp_types)\n",
    "# clean up data\n",
    "filtered_data = []\n",
    "for data_line in dicts:\n",
    "    valid = True\n",
    "    for key,item in data_line.items():\n",
    "        if type(item) != str:\n",
    "            valid = False\n",
    "    if valid:\n",
    "        filtered_data.append(data_line)\n",
    "print(len(filtered_data))\n",
    "medium_dataset = Dataset.from_list(filtered_data[:5000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e19b8e5-e21f-41fb-b862-c4a83b28efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 90732.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# process data for training https://huggingface.co/docs/datasets/en/process\n",
    "# split first so there's no leaking\n",
    "medium_dataset = medium_dataset.filter(\n",
    "    lambda example: (len(example['text']) >= 500) and\n",
    "    (len(example['title']) >= 20)\n",
    ")\n",
    "medium_dataset = medium_dataset.train_test_split(test_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b75f1c-5c19-4cb6-868a-538cf1be2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/agent_h/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# process for training\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/data/agent_h/llms/umt5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/agent_h/llms/umt5-small\")\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    add \\n to sentences, remove title\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text.strip())\n",
    "    sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
    "    sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n",
    "                                 if len(sent) > 0 and\n",
    "                                 sent[-1] in string.punctuation]\n",
    "    text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
    "    return text_cleaned\n",
    "\n",
    "#pprint(clean_text(medium_dataset['train'][0]['text']))\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    \"turn into tokens for labels and input_ids\"\n",
    "    texts_cleaned = [clean_text(text) for text in examples[\"text\"]]\n",
    "    inputs = [prefix + text for text in texts_cleaned]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"title\"], max_length=max_target_length, \n",
    "                           truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# examples = medium_dataset['test'][:100]\n",
    "# tmp_data = preprocess_data(examples)\n",
    "# print(examples['text'][8])\n",
    "# tokenizer.decode(tmp_data['input_ids'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82e0694-eb69-480b-9250-517a5d42b2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                        | 0/3627 [00:00<?, ? examples/s]/home/agent_h/miniconda3/envs/vsr3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3627/3627 [00:05<00:00, 685.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 703.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = medium_dataset.map(preprocess_data,\n",
    "                                        batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ea4a58-4ce4-42e0-b51f-0cb16937b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agent_h/miniconda3/envs/vsr3/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/agent_h/miniconda3/envs/vsr3/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /home/agent_h/.cache/huggingface/modules/datasets_modules/metrics/rouge/457c405cab0bd19db749b46bf15a1a3cff4d54f50e7ab868c293e5ece288425e (last modified on Tue May 21 16:40:41 2024) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "batch_size = 8\n",
    "base_model = \"/data/agent_h/llms/umt5-small\"\n",
    "model_name = \"umt5-small-medium-title-generation\"\n",
    "model_dir = f\"/data/agent_h/checkpoints/{model_name}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600377f8-ecb4-4bd0-b3db-c9384f003579",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7b6aa-d036-4a6c-a543-d0610308252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "#https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672d2ab-25c3-4c26-bce5-6e421f2f68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/data/agent_h/llms/umt5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/agent_h/llms/umt5-small\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"å›½å®¶\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cca08909-2b74-4bc7-a358-ca20e76acb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆæ ‡é¢˜ï¼šæ®æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€è¿‘æ—¥å…¬å‘Šï¼Œå®‰å¾½æ™¶å¥‡ç½‘ç»œç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸åœ¨ä¸­å›½è¯ç›‘ä¼šå®¡é˜…å…¶IPOå¹¶åœ¨åˆ›ä¸šæ¿ä¸Šå¸‚ç”³è¯·æ–‡ä»¶çš„è¿‡ç¨‹ä¸­ï¼Œè¯¥å…¬å¸ä¸å…¶ä¿èæœºæ„ä¸»åŠ¨è¦æ±‚æ’¤å›æ³¨å†Œç”³è¯·æ–‡ä»¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ä¼ä¸šæ—©åœ¨2021å¹´å°±å·²â€œè¿‡ä¼šâ€ã€‚æ­¤å¤–ï¼Œæµ™æ±Ÿæ§é˜€2022å¹´12æœˆåˆ›ä¸šæ¿IPOè¿‡ä¼šï¼Œä¸€å¹´å¤šæœªæäº¤æ³¨å†Œï¼Œä»Šå¹´3æœˆæ’¤å›IPOï¼›åšè±ç”µå™¨2022å¹´11æœˆåˆ›ä¸šæ¿IPOè¿‡ä¼šï¼Œè¿‡ä¼šé€¾ä¸€å¹´æœªæäº¤æ³¨å†Œï¼Œæœ€ç»ˆä»Šå¹´3æœˆæ’¤å›IPOã€‚ä¸­å›½äººæ°‘å¤§å­¦ä¸­å›½èµ„æœ¬å¸‚åœºç ”ç©¶é™¢è”å¸­é™¢é•¿èµµé”¡å†›åœ¨æ¥å—ä¸­æ–°ç¤¾ç›´é€šè½¦è®°è€…é‡‡è®¿æ—¶è¡¨ç¤ºï¼Œä»å…¬å¼€èµ„æ–™æ¥çœ‹ï¼Œä¸Šè¿°ä¼ä¸šä¹‹æ‰€ä»¥ä¸»åŠ¨æ’¤å›IPOç”³è¯·ï¼Œä¸»è¦æ˜¯åœ¨IPOè‡ªæŸ¥è¿‡ç¨‹ä¸­ï¼Œå‘ç°å…¬å¸åœ¨åˆè§„ã€æ¿å—å®šä½ã€ä¿¡æ¯æŠ«éœ²ã€ä¼šè®¡å¤„ç†ç­‰æ–¹é¢å­˜åœ¨é—®é¢˜ï¼ŒåŠæ—¶çº æ­£ã€‚ä¼ä¸šä¹‹æ‰€ä»¥åœ¨IPOé—®é¢˜ä¸Šå¦‚æ­¤ç§¯æè‡ªæŸ¥è‡ªçº ï¼Œè¿™ä¸å½“å‰ä¸­å›½èµ„æœ¬å¸‚åœºâ€œä¸¥ç›‘ç®¡â€çš„é£æ°”å¯†åˆ‡ç›¸å…³ã€‚\n",
      "output: \n",
      "['<pad> æ™¶å¥‡ç½‘ç»œç§‘æŠ€å…¬å¸ä¸»åŠ¨æ’¤å›IPOç”³è¯·</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"/data/agent_h/checkpoints/umt5-small-medium-title-generation/checkpoint-32000\"\n",
    "model_ckpt = \"/data/agent_h/checkpoints/umt5-small-medium-title-generation-zh/checkpoint-54000\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "text = \"\"\"summarize: Combining both modern and traditional style architectures, with one side of the city being modernized and renovated to fit the times, and the other half still offering traditional hutong districts.[18] Beijing is one of the oldest cities in the world, with a rich history dating back over three millennia. As the last of the Four Great Ancient Capitals of China, Beijing has been the political center of the country for most of the past eight centuries,[19] and was the largest city in the world by population for much of the second millennium CE.[20] With mountains surrounding the inland city on three sides, in addition to the old inner and outer city walls, Beijing was strategically poised and developed to be the residence of the emperor and thus was the perfect location for the imperial capital. The city is renowned for its opulent palaces, temples, parks, gardens, tombs, walls and gates.[21] Beijing is one of the most important tourist destinations of the world. In 2018, Beijing was the second highest earning tourist city in the world after Shanghai.[22] Beijing is home to many national monuments and museums and has seven UNESCO World Heritage Sitesâ€”the Forbidden City, Temple of Heaven, Summer Palace, Ming Tombs, Zhoukoudian Peking Man Site, and parts of the Great Wall and the Grand Canalâ€”all of which are popular tourist locations.[23] Siheyuans, the city's traditional housing style, and hutongs, the narrow alleys between siheyuans, are major tourist attractions and are common in urban Beijing.\"\"\"\n",
    "text = \"\"\"ç”Ÿæˆæ ‡é¢˜ï¼šè¿‘æ—¥ï¼Œä¸–ç•Œè´¸æ˜“ç»„ç»‡(WTO)å¯¹ä¸­å›½ç¬¬å…­æ¬¡è´¸æ˜“æ”¿ç­–å®¡è®®åœ¨æ—¥å†…ç“¦é¡ºåˆ©ç»“æŸã€‚æ­¤æ¬¡å®¡è®®è¿‡ç¨‹ä¸­ï¼Œä¸­å›½ç»è´¸ä½“åˆ¶ã€è´¸æ˜“æŠ•èµ„é¢†åŸŸå–å¾—çš„æ–°è¿›å±•ç­‰å¤šæ–¹é¢å¾—åˆ°ç§¯æè¯„ä»·ï¼Œå„æˆå‘˜å¯¹ä¸­å›½æˆä¸ºå…¶é‡è¦ç»è´¸åˆä½œä¼™ä¼´ååˆ†é‡è§†ã€‚å¯¹æ­¤ï¼Œä¸“å®¶æŒ‡å‡ºï¼Œè™½ç„¶ä¸ŠåŠå¹´ä¸­å›½è¿›å‡ºå£åŒä¸‹é™ï¼Œä½†ä¸­å›½åœ¨å…¨çƒè´¸æ˜“ç»æµä¸­çš„åœ°ä½ä»ä¸æ–­ä¸Šå‡ï¼Œå°¤å…¶æ˜¯ä¸­å›½å¤–è´¸æ–°æ—§åŠ¨èƒ½è½¬æ¢é‡Šæ”¾å‡ºçš„å¼ºåŠ²åŠ¨åŠ›ï¼Œå°†æ¨åŠ¨å¯¹å¤–è´¸æ˜“ç»§ç»­å›ç¨³ã€å‘å¥½ï¼Œä¹Ÿå°†ä¸ºå…¨çƒè´¸æ˜“å¢é•¿ä½œå‡ºé‡è¦è´¡çŒ®ã€‚æ–°åŠ¨èƒ½æŒç»­ç§¯ç´¯ä¼˜åŠ¿ä¸ŠåŠå¹´ï¼Œæˆ‘å›½è¿›å‡ºå£åŒæ¯”ä¸‹é™3.3%ï¼Œè¿›å£ã€å‡ºå£åˆ†åˆ«ä¸‹é™4.7%å’Œ2.1%ã€‚è™½ç„¶è¿›å‡ºå£åŒä¸‹é™ï¼Œä½†æˆ‘å›½å¤–è´¸æ–°æ—§åŠ¨èƒ½è½¬æ¢æ­£åŠ å¿«è¿›ç¨‹ï¼Œè´¸æ˜“ç»“æ„ä¸æ–­ä¼˜åŒ–ã€‚æµ·å…³æ€»ç½²æ•°æ®æ˜¾ç¤ºï¼Œ1-6æœˆï¼Œæˆ‘å›½ä¸€èˆ¬è´¸æ˜“è¿›å‡ºå£å è¿›å‡ºå£æ€»å€¼çš„56.4%ï¼Œæ¯”å»å¹´åŒæœŸæå‡1.2ä¸ªç™¾åˆ†ç‚¹;æ°‘ä¼å‡ºå£å¢é•¿3.6%ï¼Œå å‡ºå£æ€»å€¼çš„46.6%ï¼Œå æ¯”ç»§ç»­ä¿æŒé¦–ä½ã€‚â€œä¸€èˆ¬è´¸æ˜“å æ¯”çš„æŒç»­ä¸Šå‡ä½“ç°å‡ºä¸­å›½è‡ªä¸»äº§å“çš„æ¯”é‡åœ¨ä¸Šå‡ã€è‡ªä¸»åˆ›æ–°èƒ½åŠ›åœ¨å¢å¼ºï¼Œæˆ‘å›½å¯¹å¤–è´¸æ˜“æ­£å‘é«˜é™„åŠ å€¼ç«¯å‘å±•ï¼Œå¦‚é«˜æ–°æŠ€æœ¯äº§ä¸šç­‰æ–°ä¸šæ€åœ¨æˆ‘å›½å¤–è´¸å‘å±•ä¸­çš„åŠ¿å¤´å·²è¶Šæ¥è¶Šå¼ºåŠ²ã€‚è€Œæ°‘ä¼å‡ºå£çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†æ›´å¤šæ´»åŠ›ï¼Œå¤–è´¸ä¸­çš„å›½å†…èµ„æœ¬å’ŒæŠ•å…¥å“çš„å¢åŠ ï¼Œä¸ºæˆ‘å›½å¤–è´¸å¥åº·å‘å±•åŠç»“æ„ä¼˜åŒ–æä¾›äº†æ–°åŠ¨èƒ½ã€‚â€å›½å®¶å‘æ”¹å§”å¯¹å¤–ç»æµç ”ç©¶æ‰€å›½é™…åˆä½œå®¤ä¸»ä»»å¼ å»ºå¹³åœ¨æ¥å—æœ¬æŠ¥è®°è€…é‡‡è®¿æ—¶è¯´ã€‚å¤šè¾¹ã€åŒè¾¹ç»è´¸åˆä½œä¸æ–­æ‹“å±•åˆ™ä¸ºæˆ‘å›½å¤–è´¸æä¾›äº†æ›´å¤§å‘å±•ç©ºé—´ã€‚æµ·å…³æ€»ç½²æ–°é—»å‘è¨€äººé»„é¢‚å¹³æŒ‡å‡ºï¼Œä¸ŠåŠå¹´ï¼Œæˆ‘å›½å¯¹éƒ¨åˆ†â€œä¸€å¸¦ä¸€è·¯â€æ²¿çº¿å›½å®¶å‡ºå£å¢é•¿ã€‚å¦å¤–ï¼Œå·²æœ‰22ä¸ªå›½å®¶æˆ–åœ°åŒºä¸æˆ‘å›½ç­¾ç½²å¹¶å®æ–½è‡ªè´¸åå®šï¼Œä¸ŠåŠå¹´ï¼Œä¸ä¸Šè¿°å›½å®¶æˆ–åœ°åŒºçš„è¿›å‡ºå£è¡¨ç°å¥½äºåŒæœŸæˆ‘å›½è¿›å‡ºå£æ€»ä½“é™å¹…ã€‚å•†åŠ¡éƒ¨ç ”ç©¶é™¢å›½é™…å¸‚åœºç ”ç©¶éƒ¨å‰¯ä¸»ä»»ç™½æ˜è¡¨ç¤ºï¼Œä¸ŠåŠå¹´ï¼Œå¤§å‹æˆå¥—äº§å“å‡ºå£ä¿æŒæ­£å¢é•¿ï¼Œè¿™ä¸ªé¢†åŸŸçš„å•†å“æŠ€æœ¯å«é‡é«˜ï¼Œé™„åŠ å€¼ä¹Ÿæ¯”è¾ƒé«˜ï¼Œè·Ÿä¸€èˆ¬çš„ä¼ ç»Ÿå•†å“ç›¸æ¯”ï¼Œå®ƒæ›´æ˜¯æˆ‘ä»¬å‘å±•çš„ä¸€ä¸ªæ–¹å‘ã€‚è·¨å¢ƒç”µå•†è´¸æ˜“ã€å¸‚åœºé‡‡è´­è´¸æ˜“ç­‰æ–°å‹å¤–è´¸å•†ä¸šæ¨¡å¼æ­£æˆä¸ºæ–°çš„å¤–è´¸å¢é•¿ç‚¹ã€‚ä¸­å›½è´¸ä¿ƒä¼šå‰¯ä¼šé•¿å°¹å®—åæŒ‡å‡ºï¼Œå»å¹´æˆ‘å›½çš„è·¨å¢ƒç”µå­å•†åŠ¡è§„æ¨¡ä¸º5.4ä¸‡äº¿å…ƒäººæ°‘å¸ï¼Œé¢„è®¡ä»Šå¹´å¯èƒ½ä¼šè¾¾åˆ°6.5ä¸‡äº¿å…ƒäººæ°‘å¸ï¼Œå¯¹äºä¿ƒè¿›å¤–è´¸ç¨³å¢é•¿ã€è°ƒç»“æ„å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚â€œæœºå™¨æ¢äººâ€é™ä½æˆæœ¬éšç€æ–°æ—§åŠ¨èƒ½è½¬æ¢çš„æŒç»­æ¨è¿›ï¼Œä¼ ç»ŸåŠ¨èƒ½è¿™ä¸€æ›¾ç»çš„å¤–è´¸ä¸»è¦è´¡çŒ®è€…æ­£é¢ä¸´å›°å¢ƒã€‚æµ·å…³æ€»ç½²æ•°æ®æ˜¾ç¤ºï¼Œæˆªè‡³ä»Šå¹´6æœˆä»½ï¼Œæˆ‘å›½çš„åŠ å·¥è´¸æ˜“è¿›å£ã€å‡ºå£å·²ç»åˆ†åˆ«è¿ç»­18ä¸ªæœˆå’Œ16ä¸ªæœˆä¸‹é™ã€‚ä»Šå¹´ä¸ŠåŠå¹´ï¼ŒåŠ å·¥è´¸æ˜“è¿›å‡ºå£ä¸‹é™9.8%ï¼Œæ‹–ç´¯æˆ‘å›½å¤–è´¸è¿›å‡ºå£æ•´ä½“ä¸‹é™çº¦3ä¸ªç™¾åˆ†ç‚¹ã€‚â€œåŠ å·¥è´¸æ˜“ç­‰ä¼ ç»ŸåŠ¨èƒ½å¯¹äºå½“å‰çš„ä¸­å›½å¤–è´¸è€Œè¨€ä»å¾ˆé‡è¦ï¼Œå®ƒæ—¢å¯ä»¥æ¨åŠ¨è´¸æ˜“å‡è¡¡å‘å±•ï¼Œä¹Ÿèƒ½å¤Ÿä¸ºå°±ä¸šæä¾›ä¿éšœã€‚å¯¹äºåŠ å·¥è´¸æ˜“çš„ä¸‹é™ï¼Œæˆ‘ä»¬åœ¨é¡ºåº”å¸‚åœºè§„å¾‹çš„å‰æä¸‹ï¼Œè¿˜è¦å……åˆ†å‘æ˜å…¶æ½œåŠ›ï¼Œè¿™åŒ…æ‹¬äº†ç»“æ„çš„æ”¹å–„ä¸é‡çš„å¢é•¿ã€‚â€å¼ å»ºå¹³è¯´ï¼Œè€Œè¦å……åˆ†å‘æ˜è¿™ä¸€æ½œåŠ›ï¼Œåˆ™è¦åœ¨ä¿ç•™ä¼˜åŠ¿çš„åŸºç¡€ä¸Šæé«˜å…¶åœ¨ä»·å€¼é“¾ä¸­çš„åœ°ä½ï¼Œå¹¶ä¸æ–­æé«˜è´¸æ˜“ä¾¿åˆ©åŒ–æ°´å¹³ã€‚ \"\"\"\n",
    "text = \"\"\"ç”Ÿæˆæ ‡é¢˜ï¼šæ®æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€è¿‘æ—¥å…¬å‘Šï¼Œå®‰å¾½æ™¶å¥‡ç½‘ç»œç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸åœ¨ä¸­å›½è¯ç›‘ä¼šå®¡é˜…å…¶IPOå¹¶åœ¨åˆ›ä¸šæ¿ä¸Šå¸‚ç”³è¯·æ–‡ä»¶çš„è¿‡ç¨‹ä¸­ï¼Œè¯¥å…¬å¸ä¸å…¶ä¿èæœºæ„ä¸»åŠ¨è¦æ±‚æ’¤å›æ³¨å†Œç”³è¯·æ–‡ä»¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ä¼ä¸šæ—©åœ¨2021å¹´å°±å·²â€œè¿‡ä¼šâ€ã€‚æ­¤å¤–ï¼Œæµ™æ±Ÿæ§é˜€2022å¹´12æœˆåˆ›ä¸šæ¿IPOè¿‡ä¼šï¼Œä¸€å¹´å¤šæœªæäº¤æ³¨å†Œï¼Œä»Šå¹´3æœˆæ’¤å›IPOï¼›åšè±ç”µå™¨2022å¹´11æœˆåˆ›ä¸šæ¿IPOè¿‡ä¼šï¼Œè¿‡ä¼šé€¾ä¸€å¹´æœªæäº¤æ³¨å†Œï¼Œæœ€ç»ˆä»Šå¹´3æœˆæ’¤å›IPOã€‚ä¸­å›½äººæ°‘å¤§å­¦ä¸­å›½èµ„æœ¬å¸‚åœºç ”ç©¶é™¢è”å¸­é™¢é•¿èµµé”¡å†›åœ¨æ¥å—ä¸­æ–°ç¤¾ç›´é€šè½¦è®°è€…é‡‡è®¿æ—¶è¡¨ç¤ºï¼Œä»å…¬å¼€èµ„æ–™æ¥çœ‹ï¼Œä¸Šè¿°ä¼ä¸šä¹‹æ‰€ä»¥ä¸»åŠ¨æ’¤å›IPOç”³è¯·ï¼Œä¸»è¦æ˜¯åœ¨IPOè‡ªæŸ¥è¿‡ç¨‹ä¸­ï¼Œå‘ç°å…¬å¸åœ¨åˆè§„ã€æ¿å—å®šä½ã€ä¿¡æ¯æŠ«éœ²ã€ä¼šè®¡å¤„ç†ç­‰æ–¹é¢å­˜åœ¨é—®é¢˜ï¼ŒåŠæ—¶çº æ­£ã€‚ä¼ä¸šä¹‹æ‰€ä»¥åœ¨IPOé—®é¢˜ä¸Šå¦‚æ­¤ç§¯æè‡ªæŸ¥è‡ªçº ï¼Œè¿™ä¸å½“å‰ä¸­å›½èµ„æœ¬å¸‚åœºâ€œä¸¥ç›‘ç®¡â€çš„é£æ°”å¯†åˆ‡ç›¸å…³ã€‚\"\"\"\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(**inputs)\n",
    "print(text)\n",
    "print(\"output: \")\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c06af-16eb-4103-9dd1-ccbcdb3b4c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
